---
title: "Predicting Strike Outcomes with LASSO Logistic Regression: An R Programming Approach to Labor Action Event Analysis"
format: pdf
header-includes:
  - \usepackage{titlesec}
  - \titleformat*{\title}{\large}
subtitle: "Group 1 Final Project Coding"
author: "Putra Farrel Azhar, Lauryn Edwards, Meilin Chen, Yanji Wang"
date: "March 17, 2024"
execute:
  message: false
  warning: false
---


# Preliminary data wrangling and cleaning

## Loading packages and set seed id


```{r message = FALSE}
# clean the environment
rm(list = ls())

# oad all the packages
library(haven)
library(tidyverse)
library(readxl)
library(here)
library(janitor)
library(zipcodeR)
library(tigris)
library(sf)
library(tidymodels)
library(yardstick)

# set the reproducibility 
set.seed(0000)

# loading the raw LAT data
data_lat <- read_excel(here("Data Raw", "Labor action tracker data 12.4.23.xlsx"))

```


## Creating a county index for the LAT dataset


```{r}
# filtering only labor actions with one location (note you should handle multiple locations
# as determined by your group) and separating latitude and longitude into new variables
data_lat <- data_lat |>
  clean_names() |>
  filter(number_of_locations == 1) |>
  mutate(lat = as.numeric(sub(",.*", "", latitude_longitude)),
         long = as.numeric(sub(".*, ", "", latitude_longitude))) |>
  select(-latitude_longitude) |>
  filter(!is.na(lat), !is.na(long))

# download the U.S. counties shape file
counties <- counties(cb = TRUE)

# convert the latitude and longitude into sf objects
data_lat <- st_as_sf(data_lat, 
                     coords = c("long", "lat"), 
                     crs = 4326)

# transform counties shape file into the same CRS as lat_long
counties <- st_transform(counties, st_crs(data_lat))

# spatial join of data with counties
data_lat <- st_join(data_lat, counties)

# find the county variable and rename it
names(data_lat)[which(names(data_lat) == "NAMELSAD")] <- "county"
```


## Aggregate the individual ACS dataset


```{r}
# merging the ACS datasets
MC <- read.csv(here("Data Raw", "acs_data_clean(MC).csv"))
YW <- read.csv(here("Data Raw", "acs_data_clean(YW).csv"))
LE <- read.csv(here("Data Raw", "acs_data_clean(LE).csv"))

LE <- LE %>% rename(county = NAME)

# create the state vairbales
MC$state <- sub('.*,\\s*', '', MC$county)
YW$state <- sub('.*,\\s*', '', YW$county)
LE$state <- sub('.*,\\s*', '', LE$county)

# trimming white spaces
MC$state <- trimws(MC$state)
YW$state <- trimws(YW$state)
LE$state <- trimws(LE$state)

# perform the initial left join
# Perform the initial left join
acs <- merge(MC, YW, by = "county", all.x = TRUE)
acs <- merge(acs, LE, by = "county", all.x = TRUE)

#remove the redundant observations when joining
acs <- acs %>% select(-contains(".x"))
acs <- acs %>% select(-contains(".y"))

# glimpsing the dataset
glimpse(acs)

#export the combined dataset
# Save the acs dataset as a CSV file
write.csv(acs, file = "combined_acs.csv", row.names = FALSE)
```


## Merging the LAT and the combined ACS dataset by counties


```{r}
# import the combined acs dataset
combined_acs <- read_csv("combined_acs.csv")

# edit the county variable
combined_acs$county <- sub(" .*", "", combined_acs$county)
data_lat$county <- sub(" .*", "", data_lat$county)

# join with tha LAT dataset
data <- left_join(data_lat, combined_acs, by = c("county" = "county", "state" = "state"))
data <- data %>% select(-contains(".x"))
data <- data %>% select(-contains(".y"))

# export the dataset
write.csv(data, file = "general_data.csv", row.names = FALSE)
```


## Removing white spaces and cleaning the variable names


```{r message = FALSE}
df <- read_csv(here("Data Raw", "general_data.csv"))

# creating a function to remove white spaces safely
safeTrim <- function(x) {
  if(is.factor(x)) { 
    as.factor(trimws(as.character(x)))
  } else if(is.character(x)) {
    trimws(x)
  } else if(is.numeric(x)) {
    x 
  } else {
    x 
  }
}

# executing the safeTrim function
df[] <- lapply(df, safeTrim)

# cleaning the column names
df_clean <- clean_names(df)
```


# Feature Engineering

## Creating a binary strike for dependent variable


```{r}
# Remove the lockout values from strike_or_protest column
df_clean <- df_clean[df_clean$strike_or_protest != "Lockout", ]

# Create a binary variable for strike
df_clean$strike_binary <- ifelse(df_clean$strike_or_protest == "Strike", 1, 0)

# Move the strike binary variable to the front
last_column <- ncol(df_clean)

df_clean <- df_clean[, c(1, last_column, 2:(last_column - 1))]
```


## Adjusting LAT event duration into a unit of 'days'


```{r}
# Define a function to convert minutes or hours to days
convert_to_days <- function(amount, unit) {
  result <- ifelse(unit == "Minutes", amount / (24 * 60),
                   ifelse(unit == "Hours", amount / 24, amount))
  return(result)}

# Apply the conversion function to the data frame
# Transfer all durations into day-based measurement
df_clean <- df_clean %>%
  mutate(duration_amount = ifelse(duration_unit %in% c("Minutes", "Hours"),
                                  convert_to_days(duration_amount, duration_unit),
                                  duration_amount))

# Changing all NAs in duration_amount to a value of 1-day
df_clean$duration_amount[is.na(df_clean$duration_amount)] <- 1
```


## Modifying timestamp, start date, and end date as dates


```{r}
df_clean$start_date <- as.Date(df_clean$start_date, format = "%Y-%m-%d")

df_clean$end_date <- as.Date(df_clean$end_date, format = "%Y-%m-%d")

```


## Removing unnecessary and redundant columns


```{r}
df_sub <- select(df_clean, -c(number_of_locations,
                              duration_unit,
                              source,
                              notes,
                              geometry,
                              statefp,
                              countyfp,
                              countyns,
                              affgeoid,
                              name,
                              county,
                              stusps,
                              state_name,
                              lsad,
                              aland,
                              awater,
                              strike_or_protest,
                              tot_m,
                              bargaining_unit_size,
                              local,
                              tot_f,
                              hh_income_med,
                              pub_assist,
                              gini,
                              labor_organization
                              ))

```


## Handling NAs in Categorical Columns


```{r}
# Changing NAs to the categorical value of "missing"
df_sub <- df_sub %>%
  mutate(
    employer = ifelse(is.na(employer), "MISSING", employer),
    industry = ifelse(is.na(industry), "MISSING", industry),
    address = ifelse(is.na(address), "MISSING", address),
    zip_code = ifelse(is.na(zip_code), "MISSING", zip_code),
    worker_demands = ifelse(is.na(worker_demands), "MISSING", worker_demands),
    authorized = ifelse(is.na(authorized), "MISSING", authorized),
    city = ifelse(is.na(city), "MISSING", city)
  )
```

```{r}
## Handling Multiple Values in Categorical Columns

df_sub$employer <- sapply(strsplit(df_sub$employer, ";"), function(x) x[1])

df_sub$industry <- sapply(strsplit(df_sub$industry, ","), function(x) x[1])

df_sub$worker_demands <- sapply(strsplit(df_sub$worker_demands, ","), function(x) x[1])
```


## Fixing the variable types


```{r}
df_sub$strike_binary <- as.factor(df_sub$strike_binary)

df_sub$authorized <- as.character(df_sub$authorized)
```


## Remove timeframe variables


```{r}
df_sub <- df_sub %>%
  select(-start_date, -end_date, -timestamp)
```


## Final Check


```{r}
# Creating a data frame to see the number of NAs by columns
na_count_df <- data.frame(column = names(df_sub),
                          na_count = colSums(is.na(df_sub)),
                          type = sapply(df_sub, class))

# Display the data frame
view(na_count_df)

# export the dataset
write.csv(df_sub, file = "final_data.csv", row.names = FALSE)
```


# LASSO Logistic workflow

## Splitting the dataset


```{r}
world_split <- initial_split(df_sub, prop = 0.8, strata = strike_binary)
world_train <- training(world_split)
world_test <- testing(world_split)
```


## Formulating the recipe


```{r}
world_recipe <- 
  recipe(strike_binary ~ ., data = world_train) |> 
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_novel(all_nominal(), -all_outcomes()) |>
  step_impute_median(all_numeric()) |>
  step_scale(all_numeric()) |>
  step_center(all_numeric())

```


## Create the workflow


```{r}
lasso_wf <- workflow() |> 
  add_recipe(world_recipe)
```


# LASSO Logistic model using a predetermined penalty value of 0.5

## Specifying the type of model


```{r}
# Model specification using logistic regression with LASSO penalty
lasso_model <- logistic_reg(penalty = 0.5, mixture = 1) %>%
  set_engine("glmnet")
```


## Lasso Logistic Fit


```{r}
lasso_fit <- lasso_wf |> 
  add_model(lasso_model) |> 
  fit(data = world_train)
```


## Lasso Logistic Estimates


```{r}
lasso_estimates <- lasso_fit |> 
  extract_fit_parsnip() |> 
  tidy() |> 
  print()

lasso_estimates |> 
  filter(estimate == 0) |> 
  nrow()

```


# LASSO Logistic model using tune spec to find the best 'penalty' parameter

## Specifying the type of model


```{r}
# Model specification using logistic regression with LASSO penalty
tune_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_engine("glmnet")
```


## Generating folds for cross validation and penalty parameters


```{r}
# the sequence of penalty parameters to search over
penalty_grid <- tibble(
  penalty = seq(0, 2, by = .01)
)

# Generate folds for cross validation
folds <- vfold_cv(world_train, v = 5)
```

```{r}
doParallel::registerDoParallel()

lasso_grid <- tune_grid(
  lasso_wf |> add_model(tune_spec),
  resamples = folds,   # 5 fold cross validation
  grid = penalty_grid
)
```

```{r}
# collect_metrics() lets us look at the predictive output of our models
lasso_grid_metrics <- lasso_grid |>
  collect_metrics() |>
  print()
```

```{r}
lasso_grid_metrics |>
  filter(.metric == "accuracy") |> 
  ggplot(aes(x = penalty, y = mean)) +
  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), 
                alpha = 0.5) +
  geom_line(size = 1.5) +
  scale_x_log10() +
  labs(x = "Penalty", y = "Accuracy")
```


## LASSO Logistic model with the lowest RMSE


```{r}
best_accuracy <- lasso_grid |>
  select_best("accuracy")

final_lasso <- finalize_workflow(
  lasso_wf |> add_model(tune_spec),
  best_accuracy
)
```


## See which variables are most important for the prediction model


```{r}
final_lasso |>
  fit(world_train) |>
  extract_fit_parsnip() |>
  vip::vi(lambda = best_accuracy$penalty) |>
  mutate(
    Importance = abs(Importance),
    Variable = forcats::fct_reorder(Variable, Importance)
  ) |>
  filter(Importance != 0) -> vi_data

# Select top 3 positive
top_positive <- vi_data %>%
  filter(Sign == "POS") %>%
  slice_max(order_by = Importance, n = 3)

# Select top 3 negative
top_negative <- vi_data %>%
  filter(Sign == "NEG") %>%
  slice_max(order_by = Importance, n = 3)

# Combine top positive and negative
top_vi <- bind_rows(top_positive, top_negative) %>%
  mutate(Variable = forcats::fct_reorder(Variable, Importance))

# Plot
ggplot(top_vi, aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col() +
  scale_fill_manual(values = c("NEG" = "red", "POS" = "lightblue")) +
  coord_flip() +  # Flip the plot to make variable names horizontal
  theme_minimal() +
  theme(
    axis.title = element_text(size = 12),  # Adjust title size if necessary
    axis.text.x = element_text(size = 12),  # Adjust text size if necessary
    axis.text.y = element_text(size = 12, hjust = 0),  # Left-align y-axis labels and adjust size
    plot.margin = unit(c(1, 1, 1, 3), "cm")  # Increase the right margin size
  ) +
  labs(y = NULL, fill = "Sign") +
  scale_x_continuous(expand = c(0, 0)) +
  guides(fill = guide_legend(title = "Sign", title.position = "top"))
```


## Final test and compare against the held-out testing set


```{r}
# Using the best LASSO model
last_fit(
  final_lasso,
  world_split
) |> 
  collect_metrics()
```


## Confusion matrices


```{r}
# Fit the final LASSO model and predict
final_results <- last_fit(
  final_lasso,
  world_split
)

# Extract the predictions
test_predictions <- final_results %>%
  collect_predictions()

# Create a confusion matrix
conf_matrix <- test_predictions %>%
  conf_mat(truth = strike_binary, estimate = .pred_class)

# Print the confusion matrix
conf_matrix

# Manually calculate accuracy from the confusion matrix
total_cases_old <- sum(conf_matrix$table)
correct_predictions_old <- sum(diag(conf_matrix$table))
accuracy_rate_old <- correct_predictions_old / total_cases_old

# Print the accuracy rate
print(accuracy_rate_old)
```


# Preliminary data wrangling and cleaning on new dataset


```{r}
# Load the test dataset
data_new <- read_excel(here("Data Raw", "Labor action tracker data 2.26.24.xlsx"))
```


# Creating a county index for the new dataset


```{r}
# filtering only labor actions with one location (note you should handle multiple locations
# as determined by your group) and separating latitude and longitude into new variables
data_new <- data_new |>
  clean_names() |>
  filter(number_of_locations == 1) |>
  mutate(lat = as.numeric(sub(",.*", "", latitude_longitude)),
         long = as.numeric(sub(".*, ", "", latitude_longitude))) |>
  select(-latitude_longitude) |>
  filter(!is.na(lat), !is.na(long))

# download the U.S. counties shape file
counties <- counties(cb = TRUE)

# convert the latitude and longitude into sf objects
data_new <- st_as_sf(data_new, 
                     coords = c("long", "lat"), 
                     crs = 4326)

# transform counties shape file into the same CRS as lat_long
counties <- st_transform(counties, st_crs(data_new))

# spatial join of data with counties
data_new <- st_join(data_new, counties)

# find the county variable and rename it
names(data_new)[which(names(data_new) == "NAMELSAD")] <- "county"
```


## Data join again


```{r}
# join with tha LAT dataset
data_new <- left_join(data_new, combined_acs, 
                       by = c("NAME" = "county", "STATE_NAME" = "state"))

data_new <- data_new %>% select(-contains(".x"))
data_new <- data_new %>% select(-contains(".y"))
```


## Remove all white spaces


```{r}
# creating a function to remove white spaces safely
safeTrim <- function(x) {
  if(is.factor(x)) { 
    as.factor(trimws(as.character(x)))
  } else if(is.character(x)) {
    trimws(x)
  } else if(is.numeric(x)) {
    x 
  } else {
    x 
  }
}

# executing the safeTrim function
data_new[] <- lapply(data_new, safeTrim)

# cleaning the column names
data_new <- clean_names(data_new)
```


# Feature Engineering

## Creating a binary strike for dependent variable


```{r}
# Remove the lockout values from strike_or_protest column
data_new <- data_new[data_new$strike_or_protest != "Lockout", ]

# Create a binary variable for strike
data_new$strike_binary <- ifelse(data_new$strike_or_protest == "Strike", 1, 0)

# Move the strike binary variable to the front
last_column <- ncol(data_new)

data_new <- data_new[, c(1, last_column, 2:(last_column - 1))]
```


## Adjust duration


```{r}
# Define a function to convert minutes or hours to days
convert_to_days <- function(amount, unit) {
  result <- ifelse(unit == "Minutes", amount / (24 * 60),
                   ifelse(unit == "Hours", amount / 24, amount))
  return(result)}

# Apply the conversion function to the data frame
# Transfer all durations into day-based measurement
data_new <- data_new %>%
  mutate(duration_amount = ifelse(duration_unit %in% c("Minutes", "Hours"),
                                  convert_to_days(duration_amount, duration_unit),
                                  duration_amount))

# Changing all NAs in duration_amount to a value of 1-day
data_new$duration_amount[is.na(data_new$duration_amount)] <- 1
```


## Deal with timestamp


```{r}
data_new$start_date <- as.Date(data_new$start_date, format = "%Y-%m-%d")

data_new$end_date <- as.Date(data_new$end_date, format = "%Y-%m-%d")
```


## Keep only the neccesary column


```{r}
data_new <- select(data_new, c(strike_binary,
                                employer,
                                industry,
                               address,
                               city,
                               state,
                               zip_code,
                               approximate_number_of_participants,
                               duration_amount,
                               authorized,
                               worker_demands,
                               time_to_work,
                               hh_total,
                               hh_low_income,
                               population,
                               old,
                               hh_low_income_prop,
                               old_prop,
                               old_prop_high,
                               income_q,
                               ed_total,
                               other_race,
                               other_race_prop,
                               other_race_min,
                               higher_ed_prop,
                               aging_q
                              ))
```


## Remove geometry variable


```{r}
data_new <- select(data_new, -c(geometry))
```


## Handling NAs in categorical columns


```{r}
# Changing NAs to the categorical value of "missing"
data_new <- data_new %>%
  mutate(
    employer = ifelse(is.na(employer), "MISSING", employer),
    industry = ifelse(is.na(industry), "MISSING", industry),
    address = ifelse(is.na(address), "MISSING", address),
    zip_code = ifelse(is.na(zip_code), "MISSING", zip_code),
    worker_demands = ifelse(is.na(worker_demands), "MISSING", worker_demands),
    authorized = ifelse(is.na(authorized), "MISSING", authorized),
    city = ifelse(is.na(city), "MISSING", city)
  )
```


## Deal with multiple values in categorical columns


```{r}
data_new$employer <- sapply(strsplit(data_new$employer, ";"), function(x) x[1])

data_new$industry <- sapply(strsplit(data_new$industry, ","), function(x) x[1])

data_new$worker_demands <- sapply(strsplit(data_new$worker_demands, ","), function(x) x[1])

```


## Fix Variable type


```{r}
data_new$strike_binary <- as.factor(data_new$strike_binary)

data_new$authorized <- as.character(data_new$authorized)

```


## Check the data


```{r}
# Creating a data frame to see the number of NAs by columns
na_count_new <- data.frame(
  column = names(data_new),
  na_count = sapply(data_new, function(x) sum(is.na(x))),
  type = sapply(data_new, function(x) class(x)[1])
)

# Display the data frame
view(na_count_new)

# export the dataset
write.csv(data_new, file = "final_data_new.csv", row.names = FALSE)
```


# Using the tuned LASSO Logistic Model on the new dataset


```{r}
final_fit <- final_lasso |>
  fit(world_train)

# Making predictions on the testing data
  pred_new <- predict(final_fit,
                    new_data = data_new,
                    type = "class")
  
  # Binding the actual and predicted Y's together in a data frame
  df_eval <- cbind("strike_binary" = data_new$strike_binary, pred_new)
  
  # Evaluating the model with a confusion matrix
  cm <- conf_mat(data = df_eval,
           truth = strike_binary,
           estimate = .pred_class)
cm

# Collecting the mean accuracy rate for old data and new data

# Manually calculate accuracy from the confusion matrix
total_cases <- sum(cm$table)
correct_predictions <- sum(diag(cm$table))
accuracy_rate <- correct_predictions / total_cases

# Print the accuracy rate
print(accuracy_rate)
```

